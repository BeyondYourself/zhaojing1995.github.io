---
layout:     post
title:      深度学习 | 深度前馈网络
subtitle:   
date:       2019-08-16
author:     JoselynZhao
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - Deep Learning
---


## 前向
信息流过 x 的函数，流经用于 定义 f 的中间计算过程，最终到达输出 y。

## 网络
前馈神经网络被称作网络 (network) 是因为它们通常用许多不同函数复合 在一起来表示。

## 隐藏层
训练数据并没有给出 这些层中的每一层所需的输出，所以这些层被称为隐藏层 (hidden layer)。

## 神经
这些网络被称为神经网络是因为它们或多或少地受到神经科学的启发。
我们最好将前馈神经网络想成是为了实现统计泛化而设计出 的函数近似机器，它偶尔从我们了解的大脑中提取灵感但是并不是大脑功能的模型。


# 1 学习XOR
XOR 函数(‘‘异或’’ 逻辑)是两个二进制值 x1 和 x2 的运算。这些二进制值 中恰好有一个为 1 时，XOR 函数返回值为 1。其余情况下返回值为 0。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816101839846.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816102129422.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816102250265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816102322614.png)


# 2 基于梯度的学习

## 代价函数
深度神经网络设计中的一个重要方面是代价函数的选择。幸运的是，神经网络的代价函数或多或少是和其他参数模型例如线性模型的代价函数相同的
## 输出单元
代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择怎样表示输出决定了交叉熵函数的形式。

### 用于高斯输出分布的线性单元
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816103221401.png)

### 基于伯努利输出分布的 sigmoid 单元
Bernoulli 分布仅需一个单独的数来定义，神经网络只需要预测 P (y = 1 | x) 即可。为了使这个数是有效的概率，它必须处在区间 [0, 1] 中。 满足这个限制需要一些细致的设计工作。假设我们打算使用线性单元，并且通过阈值来限制它成为一个有效的概率:
![在这里插入图片描述](https://img-blog.csdnimg.cn/2019081610330972.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816103326463.png)

### 用于multinouli 输出分布的softmax单元

# 3 隐藏单元
## 整流线性单元及其扩展
整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性 单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。

它的梯度不仅大而且一致
 整流线性单元通常用于仿射变换之上:
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/2019081610384655.png)
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816103946935.png)
# 4 架构设计
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816104647400.png)

**通用近似定理** (universal approximation theorem)(Hornik et al., 1989; Cybenko, 1989) 表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何 一种 ‘‘挤压’’ 性质的激活函数(例如 logistic sigmoid 激活函数)的隐藏层，只要给 予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到 另一个有限维空间的 Borel 可测函数。

分层次： 更多的更简单的函数耦合在一起更好。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816105107342.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190816105115796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)
跳跃连接：
残差网络：防止梯度消失

用稀疏连接 代替 全连接
# 5 反向传播 和其他微分算法
![在这里插入图片描述](https://img-blog.csdnimg.cn/2019081611022154.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)
# 6 历史小计

